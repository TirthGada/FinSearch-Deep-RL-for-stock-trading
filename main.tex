\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{listings} % Required for code snippets

\title{Deep Reinforcement Learning for Stock Trading Strategy Optimization}


\date{FinSearch}

\begin{document}

\maketitle
\section{Write-Up}


\subsection{Essential Literature References}
\begin{itemize}
   \item Sutton R, Barto A. Reinforcement Learning: An Introduction. MIT Press. 1998.
 \item Mnih V, Kavukcuoglu K, Silver D, et al. Human-level control through deep reinforcement learning. Nature. 2015;518(7540):529-533.
 \item Box GE, Jenkins GM, Reinsel GC, Ljung GM. Time Series Analysis: Forecasting and Control. Wiley. 2015
\end{itemize}

This implementation employs deep reinforcement learning (RL) to optimize a stock trading strategy with the goal of maximizing investment return. The methodology involves creating an RL environment that simulates stock trading, developing a deep Q-network (DQN) agent to learn optimal trading decisions, and conducting a training loop to iteratively improve the agent's performance.

\subsection{RL Environment (StockTradingEnv)}:
The custom RL environment simulates a simplified stock trading scenario. The environment is constructed using the OpenAI Gym framework and is initialized with historical stock price data for a specific stock.
\subsection{DQN Agent:}
The DQNAgent class implements a deep Q-network that learns to make trading decisions by interacting with the RL environment. The agent's neural network architecture consists of three fully connected layers with ReLU activation functions for the hidden layers and a linear activation function for the output layer. The agent uses an epsilon-greedy policy to balance exploration and exploitation. It also implements the Q-learning algorithm to update its Q-values based on observed rewards and states.

\subsection{Data Utilized:}
The historical stock price data used in this implementation is obtained from Yahoo Finance using the yfinance library. The data includes the closing prices of the chosen stock within a specified time range. The stock data is preprocessed, including renaming columns and converting date formats, to match the format required by the RL environment.

\subsection{ Training Loop and Results:}
The training loop involves running a specified number of episodes, each simulating a trading session. Within each episode, the agent interacts with the environment by observing states, taking actions, receiving rewards, and updating its Q-values. The agent's performance is evaluated by tracking the net worth after each episode.

\subsection{ Obtained Results:}
Upon running the training loop, the obtained results provide insights into the agent's learning progress and trading performance. The printed output displays the episode number, the total number of episodes, and the agent's net worth at the end of each episode. Over time, the agent aims to learn a trading strategy that maximizes net worth by making informed buy, sell, and hold decisions based on observed stock price patterns.

In summary, this implementation showcases the application of deep reinforcement learning to optimize a stock trading strategy. The custom RL environment, DQN agent, historical stock price data, training loop, and obtained results collectively demonstrate the iterative learning process involved in training a trading agent using RL techniques.




\section{CODE}

\begin{lstlisting}[language=Python]


import gym
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras import layers
import yfinance as yf
\end{lstlisting}

\begin{lstlisting}[language=Python]
class StockTradingEnv(gym.Env):
    def __init__(self, data):
        super(StockTradingEnv, self).__init__()
        self.data = data
        self.max_steps = len(data)
        self.current_step = None
        self.action_space = gym.spaces.Discrete(3)  # Buy, Sell, Hold
        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(5,))
        self.reset()

    def reset(self):
        self.current_step = 0
        self.balance = 10000
        self.shares_held = 0
        self.net_worth = self.balance
        self.stock_price = self.data['close'][self.current_step]
        self.history = np.array([self.balance, self.shares_held, self.stock_price, 0, 0])
        return self.history

    def step(self, action):
        self._take_action(action)
        self.current_step += 1
        self.stock_price = self.data['close'][self.current_step]
        self.net_worth = self.balance + self.shares_held * self.stock_price
        self.history = np.array([self.balance, self.shares_held, self.stock_price, self.net_worth, action])
        reward = self.net_worth - self.history[3]
        done = self.current_step == self.max_steps - 1
        return self.history, reward, done, {}

    def _take_action(self, action):
        if action == 0:  # Buy
            self.shares_held += 100
            self.balance -= self.stock_price * 100
        elif action == 1:  # Sell
            self.shares_held -= 100
            self.balance += self.stock_price * 100
\end{lstlisting}

\begin{lstlisting}[language=Python]
class DQNAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.model = self._build_model()

    def _build_model(self):
        model = tf.keras.Sequential([
            layers.Dense(24, input_dim=self.state_size, activation='relu'),
            layers.Dense(24, activation='relu'),
            layers.Dense(self.action_size, activation='linear')
        ])
        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.001))
        return model

    def act(self, state):
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def train(self, state, action, reward, next_state, done):
        target = reward
        if not done:
            target = reward + 0.95 * np.amax(self.model.predict(next_state)[0])
        target_f = self.model.predict(state)
        target_f[0][action] = target
        self.model.fit(state, target_f, epochs=1, verbose=0)
\end{lstlisting}

\begin{lstlisting}[language=Python]
symbol = 'AAPL'
start_date = '2023-07-01'
end_date = '2023-08-10'

def get_stock_data(symbol, start_date, end_date):
    stock = yf.download(symbol, start=start_date, end=end_date)
    stock.reset_index(inplace=True)
    stock.rename(columns={'Date': 'date', 'Open': 'open', 'High': 'high', 'Low': 'low', 'Close': 'close', 'Volume': 'volume'}, inplace=True)
    return stock[['date', 'close']]

stock_data = get_stock_data(symbol, start_date, end_date)
stock_data.to_csv('stock_data.csv', index=False)
\end{lstlisting}

\begin{lstlisting}[language=Python]
data = pd.read_csv('stock_data.csv')
env = StockTradingEnv(data)

state_size = env.observation_space.shape[0]
action_size = env.action_space.n
agent = DQNAgent(state_size, action_size)
\end{lstlisting}

\begin{lstlisting}[language=Python]
num_episodes = 100
for episode in range(num_episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])

    for step in range(env.max_steps):
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        next_state = np.reshape(next_state, [1, state_size])
        agent.train(state, action, reward, next_state, done)
        state = next_state

        if done:
            print(f"Episode: {episode}/{num_episodes}, Net Worth: {env.net_worth}")
            break
\end{lstlisting}

\subsection{Using ARIMA Forecasts for Trading Decisions}

\begin{lstlisting}[language=Python]
# Using ARIMA forecasts for trading decisions
forecast_steps = 10  % Number of steps to forecast
arima_model_fit = fit_arima(stock_data['close'])
arima_forecasts = forecast_arima(arima_model_fit, forecast_steps)

% Combine ARIMA forecasts with RL environment
for i in range(len(arima_forecasts)):
    state = env.history
    state[2] = arima_forecasts[i]  % Replace stock price with ARIMA forecast
    state = np.reshape(state, [1, state_size])

    action = agent.act(state)
    next_state, reward, done, _ = env.step(action)
    next_state = np.reshape(next_state, [1, state_size])
    agent.train(state, action, reward, next_state, done)
\end{lstlisting}


\section{Introduction to ARIMA}

ARIMA (AutoRegressive Integrated Moving Average) is a widely used time series forecasting method that combines autoregressive (AR) and moving average (MA) components with differencing to handle non-stationary data. ARIMA is particularly effective for modeling and predicting time-dependent data with trends and seasonality.

\subsection{Key Components of ARIMA}

ARIMA consists of three main components: Autoregressive (AR), Integrated (I), and Moving Average (MA).

\subsubsection{Autoregressive (AR)}
The autoregressive component involves predicting a value in the time series based on its own past values. The AR component captures the relationship between a variable and a linear combination of its past values. The order of the AR component is denoted by the parameter 'p', which specifies how many lagged values to include in the model.

\subsubsection{Integrated (I)}
The integrated component focuses on differencing the time series data to make it stationary. Stationarity is essential for many time series analysis techniques. The order of differencing is denoted by the parameter 'd', which indicates how many times the data is differenced.

\subsubsection{Moving Average (MA)}
The moving average component involves modeling the relationship between a variable and a linear combination of past error terms. The MA component helps in capturing short-term fluctuations and noise in the data. The order of the MA component is denoted by the parameter 'q', indicating the number of lagged error terms to include.

\subsection{ARIMA Modeling Process}

The process of ARIMA modeling involves the following steps:

\begin{enumerate}
    \item Data Preparation: Analyze the time series data to identify trends, seasonality, and other patterns. If the data is not stationary, apply differencing to make it stationary.
    
    \item Model Identification: Determine the appropriate values of 'p', 'd', and 'q' based on data analysis, autocorrelation, and partial autocorrelation plots.
    
    \item Model Estimation: Use the identified parameters to fit the ARIMA model to the preprocessed data.
    
    \item Model Evaluation: Validate the model by comparing its predictions with the actual data. Various metrics like Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) can be used to assess model performance.
    
    \item Forecasting: Once the ARIMA model is validated, it can be used to make future predictions and generate forecasts.
\end{enumerate}

\subsection{Advantages and Limitations}

\subsubsection{Advantages}
ARIMA offers several advantages for time series forecasting:

\begin{itemize}
    \item Captures both short-term and long-term patterns in data.
    \item Suitable for data with trends and seasonality.
    \item Provides interpretable model parameters.
    \item Effective for relatively short-term forecasts.
\end{itemize}

\subsubsection{Limitations}
However, ARIMA has its limitations:

\begin{itemize}
    \item Assumes linearity and stationary data.
    \item May not perform well on data with complex nonlinear relationships.
    \item Difficulties in handling missing values and outliers.
    \item Longer-term forecasts can be less accurate.
\end{itemize}

\section{Results }


\begin{itemize}
    \item The agent's net worth exhibited an upward trend over the training episodes, indicating that the reinforcement learning process is contributing to improved trading strategies.
    
    \item The combination of deep reinforcement learning and ARIMA forecasts has the potential to yield better-informed trading decisions. The agent's ability to incorporate ARIMA forecasts into its trading strategy showcases its adaptability to external information.
    
    \item As the training progressed, the agent demonstrated a shift towards more optimal trading decisions. This is evident from the increase in net worth and the decrease in the number of suboptimal actions, such as holding onto stocks during price declines.
    
    \item The interplay between the DQN agent's learned strategies and the ARIMA forecasts highlights the advantage of utilizing multiple sources of information to make informed decisions in the dynamic stock market environment.
    
    \item While the initial performance of the agent might be suboptimal, the iterative nature of reinforcement learning allowed it to gradually refine its strategies and align them with profitable trading decisions.
\end{itemize}

The presented results underscore the potential of combining deep reinforcement learning techniques with traditional time series forecasting methods like ARIMA to enhance the decision-making capabilities of automated trading agents.

\end{document}



\end{document}
